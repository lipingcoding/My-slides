@inproceedings{gat,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@inproceedings{hsic,
author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch\"{o}lkopf, Bernhard},
title = {Measuring Statistical Dependence with Hilbert-Schmidt Norms},
year = {2005},
booktitle = {Proceedings of the 16th International Conference on Algorithmic Learning Theory}
}




@article{dou_decorrelate_2022,
	title = {Decorrelate Irrelevant, Purify Relevant: Overcome Textual Spurious Correlations from a Feature Perspective},
	url = {http://arxiv.org/abs/2202.08048},
	shorttitle = {Decorrelate Irrelevant, Purify Relevant},
	abstract = {Natural language understanding ({NLU}) models tend to rely on spurious correlations (i.e., dataset bias) to achieve high performance on indistribution datasets but poor performance on outof-distribution ones. Most of the existing debiasing methods often identify and weaken these samples with biased features (i.e., superﬁcial surface features that cause such spurious correlations). However, down-weighting these samples obstructs the model in learning from the non-biased parts of these samples. To tackle this challenge, in this paper, we propose to eliminate spurious correlations in a ﬁne-grained manner from a feature space perspective. Speciﬁcally, we introduce Random Fourier Features and weighted re-sampling to decorrelate the dependencies between features to mitigate spurious correlations. After obtaining decorrelated features, we further design a mutualinformation-based method to purify them, which forces the model to learn features that are more relevant to tasks. Extensive experiments on two well-studied {NLU} tasks including Natural Language Inference and Fact Veriﬁcation demonstrate that our method is superior to other comparative approaches.},
	journaltitle = {{arXiv}:2202.08048 [cs, math]},
	author = {Dou, Shihan and Zheng, Rui and Wu, Ting and Gao, Songyang and Zhang, Qi and Wu, Yueming and Huang, Xuanjing},
	urldate = {2022-04-08},
	date = {2022-02-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2202.08048},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Theory},
	file = {Dou et al. - 2022 - Decorrelate Irrelevant, Purify Relevant Overcome .pdf:/Users/king/Zotero/storage/Q6CDFV6F/Dou et al. - 2022 - Decorrelate Irrelevant, Purify Relevant Overcome .pdf:application/pdf},
}

@inproceedings{zhang_deep_2021,
	title = {Deep Stable Learning for Out-Of-Distribution Generalization},
	booktitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Zhang, Xingxuan and Cui, Peng and Xu, Renzhe and Zhou, Linjun and He, Yue and Shen, Zheyan}
	}

@article{gretton_kernel_nodate,
	title = {A Kernel Statistical Test of Independence},
	abstract = {Although kernel measures of independence have been widely applied in machine learning (notably in kernel {ICA}), there is as yet no method to determine whether they have detected statistically signiﬁcant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion ({HSIC}). The resulting test costs O(m2), where m is the sample size. We demonstrate that this test outperforms established contingency table and functional correlation-based tests, and that this advantage is greater for multivariate data. Finally, we show the {HSIC} test also applies to text (and to structured data more generally), for which no other independence test presently exists.},
	pages = {8},
	author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon H and Song, Le and Schölkopf, Bernhard and Smola, Alex J},
	langid = {english},
	file = {Gretton et al. - A Kernel Statistical Test of Independence.pdf:/Users/king/Zotero/storage/A37AZAK9/Gretton et al. - A Kernel Statistical Test of Independence.pdf:application/pdf},
}

@article{gretton_introduction_nodate,
	title = {Introduction to {RKHS}, and some simple kernel algorithms},
	pages = {33},
	author = {Gretton, Arthur},
	langid = {english},
	file = {Gretton - Introduction to RKHS, and some simple kernel algor.pdf:/Users/king/Zotero/storage/8KKLFG95/Gretton - Introduction to RKHS, and some simple kernel algor.pdf:application/pdf},
}

@inproceedings{cen_controllable_2020,
	location = {Virtual Event {CA} {USA}},
	title = {Controllable Multi-Interest Framework for Recommendation},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403344},
	doi = {10.1145/3394486.3403344},
	abstract = {Recently, neural networks have been widely used in e-commerce recommender systems, owing to the rapid development of deep learning. We formalize the recommender system as a sequential recommendation problem, intending to predict the next items that the user might be interacted with. Recent works usually give an overall embedding from a user’s behavior sequence. However, a unified user embedding cannot reflect the user’s multiple interests during a period. In this paper, we propose a novel controllable multi-interest framework for the sequential recommendation, called {ComiRec}. Our multi-interest module captures multiple interests from user behavior sequences, which can be exploited for retrieving candidate items from the large-scale item pool. These items are then fed into an aggregation module to obtain the overall recommendation. The aggregation module leverages a controllable factor to balance the recommendation accuracy and diversity. We conduct experiments for the sequential recommendation on two real-world datasets, Amazon and Taobao. Experimental results demonstrate that our framework achieves significant improvements over state-of-the-art models1. Our framework has also been successfully deployed on the offline Alibaba distributed cloud platform.},
	eventtitle = {{KDD} '20: The 26th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	pages = {2942--2951},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Cen, Yukuo and Zhang, Jianwei and Zou, Xu and Zhou, Chang and Yang, Hongxia and Tang, Jie},
	urldate = {2022-04-08},
	date = {2020-08-23},
	langid = {english},
	file = {Cen et al. - 2020 - Controllable Multi-Interest Framework for Recommen.pdf:/Users/king/Zotero/storage/IF28RTHU/Cen et al. - 2020 - Controllable Multi-Interest Framework for Recommen.pdf:application/pdf},
}

@article{bahng_learning_2020,
	title = {Learning De-biased Representations with Biased Representations},
	url = {http://arxiv.org/abs/1910.02806},
	abstract = {Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led to interesting advancement, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles), resulting in biased models that fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of a snowmobile on a desert) and the difﬁculty of quantifying or expressing biases in the ﬁrst place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to deﬁne a set of biased representations than to deﬁne and quantify bias. We demonstrate the efﬁcacy of our method across a variety of synthetic and real-world biases; our experiments show that the method discourages models from taking bias shortcuts, resulting in improved generalisation. Source code is available at https: //github.com/clovaai/rebias.},
	journaltitle = {{arXiv}:1910.02806 [cs, stat]},
	author = {Bahng, Hyojin and Chun, Sanghyuk and Yun, Sangdoo and Choo, Jaegul and Oh, Seong Joon},
	urldate = {2022-04-14},
	date = {2020-06-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.02806},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Bahng et al. - 2020 - Learning De-biased Representations with Biased Rep.pdf:/Users/king/Zotero/storage/WISLKSRK/Bahng et al. - 2020 - Learning De-biased Representations with Biased Rep.pdf:application/pdf},
}

@article{fukumizu_kernel_nodate,
	title = {Kernel Measures of Conditional Dependence},
	abstract = {We propose a new measure of conditional dependence of random variables, based on normalized cross-covariance operators on reproducing kernel Hilbert spaces. Unlike previous kernel dependence measures, the proposed criterion does not depend on the choice of kernel in the limit of inﬁnite data, for a wide class of kernels. At the same time, it has a straightforward empirical estimate with good convergence behaviour. We discuss the theoretical properties of the measure, and demonstrate its application in experiments.},
	pages = {8},
	author = {Fukumizu, Kenji and Gretton, Arthur and Sun, Xiaohai and Schölkopf, Bernhard},
	langid = {english},
	file = {Fukumizu et al. - Kernel Measures of Conditional Dependence.pdf:/Users/king/Zotero/storage/LSEEUJUE/Fukumizu et al. - Kernel Measures of Conditional Dependence.pdf:application/pdf},
}

@report{fukumizu_dimensionality_2003,
	location = {Fort Belvoir, {VA}},
	title = {Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces:},
	url = {http://www.dtic.mil/docs/citations/ADA446572},
	shorttitle = {Dimensionality Reduction for Supervised Learning With Reproducing Kernel Hilbert Spaces},
	abstract = {We propose a novel method of dimensionality reduction for supervised learning problems. Given a regression or classiﬁcation problem in which we wish to predict a response variable Y from an explanatory variable X, we treat the problem of dimensionality reduction as that of ﬁnding a low-dimensional “eﬀective subspace” for X which retains the statistical relationship between X and Y . We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem we establish a general nonparametric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces. This characterization allows us to derive a contrast function for estimation of the eﬀective subspace. Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y . We present experiments that compare the performance of the method with conventional methods.},
	institution = {Defense Technical Information Center},
	author = {Fukumizu, Kenji and Bach, Francis R. and Jordan, Michael I.},
	urldate = {2022-04-14},
	date = {2003-05-25},
	langid = {english},
	doi = {10.21236/ADA446572},
	file = {Fukumizu et al. - 2003 - Dimensionality Reduction for Supervised Learning W.pdf:/Users/king/Zotero/storage/7FCBXH9P/Fukumizu et al. - 2003 - Dimensionality Reduction for Supervised Learning W.pdf:application/pdf},
}

@incollection{hutchison_measuring_2005,
	location = {Berlin, Heidelberg},
	title = {Measuring Statistical Dependence with Hilbert-Schmidt Norms},
	volume = {3734},
	isbn = {978-3-540-29242-5 978-3-540-31696-1},
	url = {http://link.springer.com/10.1007/11564089_7},
	abstract = {We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces ({RKHSs}), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or {HSIC}). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-deﬁned regularisation. Second, there is a clearly deﬁned population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on {HSIC} do not suﬀer from slow learning rates. Finally, we show in the context of independent component analysis ({ICA}) that the performance of {HSIC} is competitive with that of previously published kernel-based criteria, and of other recently published {ICA} methods.},
	pages = {63--77},
	booktitle = {Algorithmic Learning Theory},
	publisher = {Springer Berlin Heidelberg},
	author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Schölkopf, Bernhard},
	editor = {Jain, Sanjay and Simon, Hans Ulrich and Tomita, Etsuji},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2022-04-15},
	date = {2005},
	langid = {english},
	doi = {10.1007/11564089_7},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Gretton et al. - 2005 - Measuring Statistical Dependence with Hilbert-Schm.pdf:/Users/king/Zotero/storage/VSR778LQ/Gretton et al. - 2005 - Measuring Statistical Dependence with Hilbert-Schm.pdf:application/pdf},
}